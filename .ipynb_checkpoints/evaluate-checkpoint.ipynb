{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451ad841-f21e-41f1-8bd7-4c8b2baa5e19",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v1.layers' has no attribute 'variance_scaling_initializer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-673016d8ca58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mdecom_output_R\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mR_decom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mdecom_output_I\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mI_decom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0moutput_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRestoration_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_low_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_low_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0moutput_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIllumination_adjust_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_low_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_low_i_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Infinova/超分辨率/KinD_plus/model.py\u001b[0m in \u001b[0;36mRestoration_net\u001b[0;34m(input_r, input_i, training)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mconv1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'de_conv1_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mconv1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'de_conv1_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mmsia_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsia_3_M\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'de_conv1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, name='de_conv1_22')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mconv2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsia_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'de_conv2_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Infinova/超分辨率/KinD_plus/msia_BN_3_M.py\u001b[0m in \u001b[0;36mmsia_3_M\u001b[0;34m(input_feature, input_i, name, training)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmsia_3_M\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mspatial_attention_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0millu_attention_3_M\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mmsia_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMulti_Scale_Module_3_M\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatial_attention_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmsia_feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Infinova/超分辨率/KinD_plus/msia_BN_3_M.py\u001b[0m in \u001b[0;36millu_attention_3_M\u001b[0;34m(input_feature, input_i, name)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#   kernel_initializer = tf.compat.v1.contrib.layers.variance_scaling_initializer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_scaling_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.compat.v1.layers' has no attribute 'variance_scaling_initializer'"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from model import *\n",
    "from glob import glob\n",
    "from skimage import color,filters\n",
    "import argparse\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "\n",
    "parser.add_argument('--save_dir', dest='save_dir', default='./test_results/', help='directory for testing outputs')\n",
    "parser.add_argument('--test_dir', dest='test_dir', default='./test_images/', help='directory for testing inputs')\n",
    "parser.add_argument('--adjustment', dest='adjustment', default=False, help='whether to adjust illumination')\n",
    "parser.add_argument('--ratio', dest='ratio', default=5.0, help='ratio for illumination adjustment')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "training = tf.compat.v1.placeholder_with_default(False, shape=(), name='training')\n",
    "input_decom = tf.compat.v1.placeholder(tf.compat.v1.float32, [None, None, None, 3], name='input_decom')\n",
    "input_low_r = tf.compat.v1.placeholder(tf.compat.v1.float32, [None, None, None, 3], name='input_low_r')\n",
    "input_low_i = tf.compat.v1.placeholder(tf.compat.v1.float32, [None, None, None, 1], name='input_low_i')\n",
    "input_high_r = tf.compat.v1.placeholder(tf.compat.v1.float32, [None, None, None, 3], name='input_high_r')\n",
    "input_high_i = tf.compat.v1.placeholder(tf.compat.v1.float32, [None, None, None, 1], name='input_high_i')\n",
    "input_low_i_ratio = tf.compat.v1.placeholder(tf.compat.v1.float32, [None, None, None, 1], name='input_low_i_ratio')\n",
    "\n",
    "[R_decom, I_decom] = DecomNet(input_decom)\n",
    "decom_output_R = R_decom\n",
    "decom_output_I = I_decom\n",
    "output_r = Restoration_net(input_low_r, input_low_i, training)\n",
    "output_i = Illumination_adjust_net(input_low_i, input_low_i_ratio)\n",
    "\n",
    "# load pretrained model\n",
    "var_Decom = [var for var in tf.compat.v1.trainable_variables() if 'DecomNet' in var.name]\n",
    "var_adjust = [var for var in tf.compat.v1.trainable_variables() if 'I_enhance_Net' in var.name]\n",
    "var_restoration = [var for var in tf.compat.v1.trainable_variables() if 'Denoise_Net' in var.name]\n",
    "g_list = tf.compat.v1.global_variables()\n",
    "bn_moving_vars = [g for g in g_list if 'moving_mean' in g.name]\n",
    "bn_moving_vars += [g for g in g_list if 'moving_variance' in g.name]\n",
    "var_restoration += bn_moving_vars\n",
    "\n",
    "saver_Decom = tf.compat.v1.train.Saver(var_list = var_Decom)\n",
    "saver_adjust = tf.compat.v1.train.Saver(var_list=var_adjust)\n",
    "saver_restoration = tf.compat.v1.train.Saver(var_list=var_restoration)\n",
    "\n",
    "decom_checkpoint_dir ='./checkpoint/decom_model/'\n",
    "ckpt_pre=tf.compat.v1.train.get_checkpoint_state(decom_checkpoint_dir)\n",
    "if ckpt_pre:\n",
    "    print('loaded '+ckpt_pre.model_checkpoint_path)\n",
    "    saver_Decom.restore(sess,ckpt_pre.model_checkpoint_path)\n",
    "else:\n",
    "    print('No decomnet checkpoint!')\n",
    "\n",
    "checkpoint_dir_adjust = './checkpoint/illu_model/'\n",
    "ckpt_adjust=tf.compat.v1.train.get_checkpoint_state(checkpoint_dir_adjust)\n",
    "if ckpt_adjust:\n",
    "    print('loaded '+ckpt_adjust.model_checkpoint_path)\n",
    "    saver_adjust.restore(sess,ckpt_adjust.model_checkpoint_path)\n",
    "else:\n",
    "    print(\"No adjust pre model!\")\n",
    "\n",
    "checkpoint_dir_restoration = './checkpoint/restoration_model/'\n",
    "ckpt=tf.compat.v1.train.get_checkpoint_state(checkpoint_dir_restoration)\n",
    "if ckpt:\n",
    "    print('loaded '+ckpt.model_checkpoint_path)\n",
    "    saver_restoration.restore(sess,ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    print(\"No restoration pre model!\")\n",
    "\n",
    "###load eval data\n",
    "eval_low_data = []\n",
    "eval_img_name =[]\n",
    "eval_low_data_name = glob(args.test_dir+'*')\n",
    "eval_low_data_name.sort()\n",
    "for idx in range(len(eval_low_data_name)):\n",
    "    [_, name] = os.path.split(eval_low_data_name[idx])\n",
    "    suffix = name[name.find('.') + 1:]\n",
    "    name = name[:name.find('.')]\n",
    "    eval_img_name.append(name)\n",
    "    eval_low_im = load_images(eval_low_data_name[idx])\n",
    "    print(eval_low_im.shape)\n",
    "    h,w,c = eval_low_im.shape\n",
    "# the size of test image H and W need to be multiple of 4, if it is not a multiple of 4, we will discard some border pixels.  \n",
    "    h_tmp = h%4\n",
    "    w_tmp = w%4\n",
    "    eval_low_im_resize = eval_low_im[0:h-h_tmp, 0:w-w_tmp, :]\n",
    "    print(eval_low_im_resize.shape)\n",
    "    eval_low_data.append(eval_low_im_resize)\n",
    "\n",
    "sample_dir = args.save_dir \n",
    "if not os.path.isdir(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "print(\"Start evalating!\")\n",
    "start_time = time.time()\n",
    "for idx in range(len(eval_low_data)):\n",
    "    print(idx)\n",
    "    name = eval_img_name[idx]\n",
    "    input_low = eval_low_data[idx]\n",
    "    input_low_eval = np.expand_dims(input_low, axis=0)\n",
    "    h, w, _ = input_low.shape\n",
    "\n",
    "    decom_r_low, decom_i_low = sess.run([decom_output_R, decom_output_I], feed_dict={input_decom: input_low_eval})\n",
    "    restoration_r = sess.run(output_r, feed_dict={input_low_r: decom_r_low, input_low_i: decom_i_low, training: False})\n",
    "### change the ratio to get different exposure level, the value can be 0-5.0\n",
    "    ratio = float(args.ratio)\n",
    "    i_low_data_ratio = np.ones([h, w])*(ratio)\n",
    "    i_low_ratio_expand = np.expand_dims(i_low_data_ratio , axis =2)\n",
    "    i_low_ratio_expand2 = np.expand_dims(i_low_ratio_expand, axis=0)\n",
    "    adjust_i = sess.run(output_i, feed_dict={input_low_i: decom_i_low, input_low_i_ratio: i_low_ratio_expand2})\n",
    "\n",
    "#The restoration result can find more details from very dark regions, however, it will restore the very dark regions\n",
    "#with gray colors, we use the following operator to alleviate this weakness.  \n",
    "    decom_r_sq = np.squeeze(decom_r_low)\n",
    "    r_gray = color.rgb2gray(decom_r_sq)\n",
    "    r_gray_gaussion = filters.gaussian(r_gray, 3)\n",
    "    low_i =  np.minimum((r_gray_gaussion*2)**0.5,1)\n",
    "    low_i_expand_0 = np.expand_dims(low_i, axis = 0)\n",
    "    low_i_expand_3 = np.expand_dims(low_i_expand_0, axis = 3)\n",
    "    result_denoise = restoration_r*low_i_expand_3\n",
    "    fusion4 = result_denoise*adjust_i\n",
    "\n",
    "    if args.adjustment:\n",
    "        fusion = decom_i_low*input_low_eval + (1-decom_i_low)*fusion4\n",
    "    else:\n",
    "        fusion = decom_i_low*input_low_eval + (1-decom_i_low)*result_denoise\n",
    "    #fusion2 = decom_i_low*input_low_eval + (1-decom_i_low)*restoration_r\n",
    "    save_images(os.path.join(sample_dir, '%s_KinD_plus.png' % (name)), fusion)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27a81c-a860-4338-a8e4-ce52db2fd2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bac014-06d8-4481-b1a1-a17adb121bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
